{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T06:15:21.495949Z",
     "start_time": "2019-02-27T06:15:17.390675Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T06:15:21.601169Z",
     "start_time": "2019-02-27T06:15:21.497948Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ML\\Anaconda3\\envs\\me\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\Users\\ML\\Anaconda3\\envs\\me\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\Users\\ML\\Anaconda3\\envs\\me\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "mnist = datasets.fetch_mldata('MNIST original', data_home='.')\n",
    "# import scipy.io\n",
    "# mnist = scipy.io.loadmat('mnist-original.mat')\n",
    "n = len(mnist.data)\n",
    "N = 10000\n",
    "indices = np.random.permutation(range(n))[:N]\n",
    "X = mnist.data[indices]\n",
    "y = mnist.target[indices]\n",
    "Y = np.eye(10)[y.astype(int)] # 1-of-K representation\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T06:15:40.719786Z",
     "start_time": "2019-02-27T06:15:21.604177Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 0s 39us/step - loss: 1.6472 - acc: 0.5031\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 1.0003 - acc: 0.7669\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.7749 - acc: 0.8270\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.6515 - acc: 0.8542\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.5694 - acc: 0.8749\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.5147 - acc: 0.8844\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.4748 - acc: 0.8932\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.4370 - acc: 0.9020\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.4112 - acc: 0.9069\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.3895 - acc: 0.9118\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.3690 - acc: 0.9164\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.3480 - acc: 0.9203\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.3347 - acc: 0.9239\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.3188 - acc: 0.9261: 0s - loss: 0.3189 - acc: 0.92\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.3050 - acc: 0.9311\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.2934 - acc: 0.9331\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.2822 - acc: 0.9363\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.2743 - acc: 0.9393\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.2636 - acc: 0.9413\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.2536 - acc: 0.9440\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.2465 - acc: 0.9453\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.2398 - acc: 0.9474\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.2309 - acc: 0.9500\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.2241 - acc: 0.9508\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.2172 - acc: 0.9523\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.2106 - acc: 0.9543\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.2069 - acc: 0.9550\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.2002 - acc: 0.9559\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.1960 - acc: 0.9581\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.1923 - acc: 0.9574\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.1877 - acc: 0.9598\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.1820 - acc: 0.9603\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.1774 - acc: 0.9616\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.1727 - acc: 0.9641\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.1697 - acc: 0.9645\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.1663 - acc: 0.9656\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.1631 - acc: 0.9655\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.1593 - acc: 0.9671\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.1565 - acc: 0.9683\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.1533 - acc: 0.9684\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.1512 - acc: 0.9689\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.1500 - acc: 0.9683\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.1470 - acc: 0.9695\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.1440 - acc: 0.9710\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.1413 - acc: 0.9713\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.1397 - acc: 0.9709\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.1376 - acc: 0.9715\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.1357 - acc: 0.9723\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.1343 - acc: 0.9719\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.1330 - acc: 0.9724\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.1307 - acc: 0.9730\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.1297 - acc: 0.9730\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.1281 - acc: 0.9734\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.1267 - acc: 0.9733\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.1255 - acc: 0.9735\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.1241 - acc: 0.9738\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.1231 - acc: 0.9741\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.1218 - acc: 0.9740\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.1208 - acc: 0.9746: 0s - loss: 0.1281 - acc: 0.9\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.1197 - acc: 0.9743\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.1193 - acc: 0.9749\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.1179 - acc: 0.9748\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.1167 - acc: 0.9748\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 0s 20us/step - loss: 0.1159 - acc: 0.9748\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.1151 - acc: 0.9750\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.1145 - acc: 0.9753\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.1139 - acc: 0.9749\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.1130 - acc: 0.9754\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.1121 - acc: 0.9758\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.1114 - acc: 0.9756\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.1107 - acc: 0.9760\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.1099 - acc: 0.9758\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.1092 - acc: 0.9760\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.1086 - acc: 0.9761\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 0.1079 - acc: 0.9761\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.1074 - acc: 0.9764\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.1068 - acc: 0.9768\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.1061 - acc: 0.9764\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.1056 - acc: 0.9766\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.1050 - acc: 0.9766\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.1045 - acc: 0.9766\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.1042 - acc: 0.9765\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 0s 20us/step - loss: 0.1033 - acc: 0.9770\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 0s 20us/step - loss: 0.1028 - acc: 0.9769\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.1024 - acc: 0.9773\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.1018 - acc: 0.9770\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.1012 - acc: 0.9771\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.1008 - acc: 0.9771\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.1003 - acc: 0.9774\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.0999 - acc: 0.9771\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.0995 - acc: 0.9773\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.0990 - acc: 0.9773\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.0985 - acc: 0.9775\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.0981 - acc: 0.9775\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 0s 20us/step - loss: 0.0978 - acc: 0.9776\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0973 - acc: 0.9779\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.0970 - acc: 0.9774\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.0966 - acc: 0.9778\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.0965 - acc: 0.9778\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.0957 - acc: 0.9780\n",
      "2000/2000 [==============================] - 0s 28us/step\n",
      "[0.2903954864740372, 0.915]\n"
     ]
    }
   ],
   "source": [
    "n_in = len(X[0])\n",
    "n_hidden = 200\n",
    "n_out = len(Y[0])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(n_hidden, input_dim=n_in))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Dense(n_out))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=SGD(lr=0.01),\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 100\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "loss_and_metrics = model.evaluate(X_test, Y_test)\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T06:15:40.740772Z",
     "start_time": "2019-02-27T06:15:40.722784Z"
    }
   },
   "outputs": [],
   "source": [
    "model.add(Dense(n_hidden))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T06:15:40.809731Z",
     "start_time": "2019-02-27T06:15:40.742772Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(n_hidden, input_dim=n_in))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Dense(n_hidden))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Dense(n_hidden))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Dense(n_out))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T06:15:40.815727Z",
     "start_time": "2019-02-27T06:15:40.811729Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T06:15:40.850704Z",
     "start_time": "2019-02-27T06:15:40.817726Z"
    }
   },
   "outputs": [],
   "source": [
    "class DNN(object):\n",
    "    def __init__(self, n_in, n_hiddens, n_out):\n",
    "        #초기화 처리\n",
    "        self.n_in = n_in\n",
    "        self.n_hiddens = n_hiddens\n",
    "        self.n_out = n_out\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self._x = None\n",
    "        self._t = None\n",
    "        self._keep_prop = None\n",
    "        self._sess = None\n",
    "        self._history = {'accuracy': [], 'loss': []}\n",
    "\n",
    "    def weight_variable(self, shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(self, shape):\n",
    "        initial = tf.zeros(shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def inference(self, x, keep_prob):\n",
    "        #모델 정의\n",
    "        for i, n_hidden in enumerate(self.n_hiddens):\n",
    "            if i == 0:\n",
    "                input = x\n",
    "                input_dim = self.n_in\n",
    "            else:\n",
    "                input = output\n",
    "                input_dim = self.n_hiddens[i - 1]\n",
    "\n",
    "            self.weights.append(self.weight_variable([input_dim, n_hidden]))\n",
    "            self.biases.append(self.bias_variable([n_hidden]))\n",
    "\n",
    "            h = tf.nn.relu(\n",
    "                tf.matmul(input, self.weights[-1]) + self.biaases[-1])\n",
    "\n",
    "            output = tf.nn.dropout(h, keep_prob)\n",
    "\n",
    "        self.weights.append(\n",
    "            self.weight_variable([self.n_hiddens[-1], self.n_out]))\n",
    "        self.biases.append(self.bias_variable([self.n_out]))\n",
    "\n",
    "        y = tf.nn.softmax(\n",
    "            tf.matmul(output, self.weights[-1]) + self.biases[-1])\n",
    "        return y\n",
    "\n",
    "    def loss(self, y, t):\n",
    "        cross_entropy = tf.reduc_mean(\n",
    "            -tf.reduce_sum(t * tf.log(y), reduction_indices=[1]))\n",
    "        return cross_entropy0\n",
    "\n",
    "    def training(self, loss):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "        train_step = optimizer.minimize(loss)\n",
    "        return train_step\n",
    "\n",
    "    def accuracy(self, y, t):\n",
    "        correct_prediction - tf.equal(tf.argmax(y, 1), tf.argmax(t, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        return accuracy\n",
    "\n",
    "    def fit(self,\n",
    "            X_train,\n",
    "            Y_train,\n",
    "            epochs=100,\n",
    "            batch_size=100,\n",
    "            p_keep=0.5,\n",
    "            verbose=1):\n",
    "        # 학습처리\n",
    "        x = tf.placeholder(tf.float32, shape=[None, self.n_in])\n",
    "        t = tf.placeholder(tf.float32, shape=[None, self.n_out])\n",
    "        keep_prop = tf.placeholder(tf.float32)\n",
    "\n",
    "        # evaluate()용으로 작성\n",
    "        self._x = x\n",
    "        self._t = t\n",
    "        self._keep_prob = keep_prop\n",
    "\n",
    "        y = self.inference(x, keep_prob)\n",
    "        loss = self.loss(y, t)\n",
    "        train_step = self.training(loss)\n",
    "        accuracy = self.accuracy(y, t)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess = tf.Session()\n",
    "        sess.run(init)\n",
    "\n",
    "        # evalutate()용으로 작성해둔다\n",
    "        self._sess = sess\n",
    "\n",
    "        N_train = len(X_train)\n",
    "        n_batches = N_train // batch_size\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            X_, y_ = shuffle(X_train, Y_train)\n",
    "\n",
    "            for i in range(n_batches):\n",
    "                start = i * batch_size\n",
    "                end = stasrt + batch_size\n",
    "\n",
    "                sess.run(\n",
    "                    train_step,\n",
    "                    feed_dict={\n",
    "                        x: X_[start:end],\n",
    "                        t: Y_[start:end],\n",
    "                        keep_prob: p_keep\n",
    "                    })\n",
    "            loss_ = loss.eval(\n",
    "                session=sess,\n",
    "                feed_dict={\n",
    "                    x: X_train,\n",
    "                    t: Y_train,\n",
    "                    keep_prob: 1.0\n",
    "                })\n",
    "\n",
    "            accuracy_ = accuracy.eval(\n",
    "                session=sess,\n",
    "                feed_dict={\n",
    "                    x: X_train,\n",
    "                    t: Y_train,\n",
    "                    keep_prob: 1.0\n",
    "                })\n",
    "\n",
    "            # 값 기록\n",
    "            self._history['loss'].append(loss_)\n",
    "            self._history['accuracy'].append(accuracy_)\n",
    "\n",
    "            if verbose:\n",
    "                print('epoch:', epoch, ' loss:', loss_, ' accuracy:',\n",
    "                      accuracy_)\n",
    "\n",
    "        return self._history\n",
    "\n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        # 평가처리\n",
    "        return self.accuracy.eval(\n",
    "            session=self._sess,\n",
    "            feed_dict={\n",
    "                self._x: X_test,\n",
    "                self._t: Y_test,\n",
    "                self._keep_prob: 1.0\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T06:15:55.312762Z",
     "start_time": "2019-02-27T06:15:55.308765Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = DNN(n_in=784,\n",
    "#            n_hiddens=[200, 200, 200],\n",
    "#            n_out=10)\n",
    "\n",
    "# model.fit(X_train, Y_train,\n",
    "#          epochs=50,\n",
    "#          batch_size=200,\n",
    "#          p_keep=0.5)\n",
    "\n",
    "# accuracy = model.evaluate(X_test, Y_test)\n",
    "# print('accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T06:16:02.399593Z",
     "start_time": "2019-02-27T06:16:02.395596Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_size = 0.8\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=trainsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T06:16:56.506209Z",
     "start_time": "2019-02-27T06:16:56.484223Z"
    }
   },
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T06:18:29.719681Z",
     "start_time": "2019-02-27T06:18:29.715684Z"
    }
   },
   "outputs": [],
   "source": [
    "# N_train = 20000\n",
    "# N_valitaion = 4000\n",
    "\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=N_train)\n",
    "\n",
    "# X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, train_size=N_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T06:24:46.516084Z",
     "start_time": "2019-02-27T06:24:46.501094Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00377964, 0.00377964, 0.00377964, 0.00377964, 0.00377964,\n",
       "       0.00377964, 0.00377964, 0.00377964, 0.00377964, 0.00377964,\n",
       "       0.00377964, 0.00377964, 0.00377964, 0.00377964, 0.00377964,\n",
       "       0.00377964, 0.00377964, 0.00377964, 0.00377964, 0.00377964,\n",
       "       0.00377964, 0.00377964, 0.00377964, 0.00377964, 0.00377964,\n",
       "       0.00377964, 0.00377964, 0.00377964, 0.00377964, 0.00377964,\n",
       "       0.00377964, 0.00377964, 0.00377964, 0.00377964, 0.00377964,\n",
       "       0.00377964, 0.00377964, 0.00377964, 0.00377964, 0.00377964,\n",
       "       0.00377964, 0.00377964, 0.00377964, 0.00377964, 0.00377964,\n",
       "       0.00377964, 0.00377964, 0.00377964, 0.00377964, 0.00377964,\n",
       "       0.00377964, 0.00377964, 0.00377964, 0.00377964, 0.00377964,\n",
       "       0.00377964, 0.00377964, 0.00377964, 0.00377964, 0.00377964,\n",
       "       0.00377964, 0.00377964, 0.00377964, 0.00377964, 0.00377964,\n",
       "       0.00377964, 0.00377964, 0.00377964, 0.00377964, 0.00377964,\n",
       "       0.00377964, 0.00377964, 0.00377964, 0.00377964, 0.00377964,\n",
       "       0.00377964, 0.00377964, 0.00377964, 0.00377964, 0.00377964,\n",
       "       0.00377964, 0.00377964, 0.00377964, 0.00377964, 0.00377964,\n",
       "       0.00377964, 0.00377964, 0.00377964, 0.00377964, 0.00377964,\n",
       "       0.00377964, 0.00377964, 0.00377964, 0.00377964, 0.00377964,\n",
       "       0.00377964, 0.00377964, 0.00377964, 0.00377964, 0.00377964])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "shape = 100\n",
    "#LeCun et al. 1988\n",
    "np.random.uniform(low=np.sqrt(1.0/n),\n",
    "                  high=np.sqrt(1.0 / n),\n",
    "                  size=shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T06:26:08.236160Z",
     "start_time": "2019-02-27T06:26:08.229164Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08692914, 0.08692914, 0.08692914, 0.08692914, 0.08692914,\n",
       "       0.08692914, 0.08692914, 0.08692914, 0.08692914, 0.08692914,\n",
       "       0.08692914, 0.08692914, 0.08692914, 0.08692914, 0.08692914,\n",
       "       0.08692914, 0.08692914, 0.08692914, 0.08692914, 0.08692914,\n",
       "       0.08692914, 0.08692914, 0.08692914, 0.08692914, 0.08692914,\n",
       "       0.08692914, 0.08692914, 0.08692914, 0.08692914, 0.08692914,\n",
       "       0.08692914, 0.08692914, 0.08692914, 0.08692914, 0.08692914,\n",
       "       0.08692914, 0.08692914, 0.08692914, 0.08692914, 0.08692914,\n",
       "       0.08692914, 0.08692914, 0.08692914, 0.08692914, 0.08692914,\n",
       "       0.08692914, 0.08692914, 0.08692914, 0.08692914, 0.08692914,\n",
       "       0.08692914, 0.08692914, 0.08692914, 0.08692914, 0.08692914,\n",
       "       0.08692914, 0.08692914, 0.08692914, 0.08692914, 0.08692914,\n",
       "       0.08692914, 0.08692914, 0.08692914, 0.08692914, 0.08692914,\n",
       "       0.08692914, 0.08692914, 0.08692914, 0.08692914, 0.08692914,\n",
       "       0.08692914, 0.08692914, 0.08692914, 0.08692914, 0.08692914,\n",
       "       0.08692914, 0.08692914, 0.08692914, 0.08692914, 0.08692914,\n",
       "       0.08692914, 0.08692914, 0.08692914, 0.08692914, 0.08692914,\n",
       "       0.08692914, 0.08692914, 0.08692914, 0.08692914, 0.08692914,\n",
       "       0.08692914, 0.08692914, 0.08692914, 0.08692914, 0.08692914,\n",
       "       0.08692914, 0.08692914, 0.08692914, 0.08692914, 0.08692914])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Glorot and Bengio 2010\n",
    "np.random.uniform(low=np.sqrt(6.0 / (n_in + n_out)),\n",
    "                  high=np.sqrt(6.0 / (n_in + n_out)),\n",
    "                  size=shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "me",
   "language": "python",
   "name": "me"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
