{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T04:39:53.273104Z",
     "start_time": "2019-02-21T04:39:53.269107Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T04:39:53.400025Z",
     "start_time": "2019-02-21T04:39:53.275104Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ML\\Anaconda3\\envs\\me\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\Users\\ML\\Anaconda3\\envs\\me\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "mnist = datasets.fetch_mldata('MNIST original', data_home='.')\n",
    "# import scipy.io\n",
    "# mnist = scipy.io.loadmat('mnist-original.mat')\n",
    "n = len(mnist.data)\n",
    "N = 10000\n",
    "indices = np.random.permutation(range(n))[:N]\n",
    "X = mnist.data[indices]\n",
    "y = mnist.target[indices]\n",
    "Y = np.eye(10)[y.astype(int)] # 1-of-K representation\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T04:41:13.541032Z",
     "start_time": "2019-02-21T04:39:53.402025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 1.6818 - acc: 0.4971\n",
      "Epoch 2/1000\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 1.0192 - acc: 0.7630\n",
      "Epoch 3/1000\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.7851 - acc: 0.8244\n",
      "Epoch 4/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.6533 - acc: 0.8568\n",
      "Epoch 5/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.5712 - acc: 0.8719\n",
      "Epoch 6/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.5147 - acc: 0.8888\n",
      "Epoch 7/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.4724 - acc: 0.8935\n",
      "Epoch 8/1000\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.4346 - acc: 0.9035\n",
      "Epoch 9/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.4049 - acc: 0.9098\n",
      "Epoch 10/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.3762 - acc: 0.9181\n",
      "Epoch 11/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.3590 - acc: 0.9200\n",
      "Epoch 12/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.3411 - acc: 0.9249\n",
      "Epoch 13/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.3252 - acc: 0.9259\n",
      "Epoch 14/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.3072 - acc: 0.9333\n",
      "Epoch 15/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.2967 - acc: 0.9341\n",
      "Epoch 16/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.2845 - acc: 0.9366\n",
      "Epoch 17/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.2729 - acc: 0.9414\n",
      "Epoch 18/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.2599 - acc: 0.9441\n",
      "Epoch 19/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.2500 - acc: 0.9471\n",
      "Epoch 20/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.2419 - acc: 0.9478\n",
      "Epoch 21/1000\n",
      "8000/8000 [==============================] - 0s 37us/step - loss: 0.2340 - acc: 0.9484\n",
      "Epoch 22/1000\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 0.2264 - acc: 0.9524\n",
      "Epoch 23/1000\n",
      "8000/8000 [==============================] - 0s 38us/step - loss: 0.2199 - acc: 0.9537\n",
      "Epoch 24/1000\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.2111 - acc: 0.9551\n",
      "Epoch 25/1000\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.2049 - acc: 0.9579\n",
      "Epoch 26/1000\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.1995 - acc: 0.9594\n",
      "Epoch 27/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.1925 - acc: 0.9609\n",
      "Epoch 28/1000\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 0.1879 - acc: 0.9616\n",
      "Epoch 29/1000\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.1820 - acc: 0.9608\n",
      "Epoch 30/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.1763 - acc: 0.9644\n",
      "Epoch 31/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.1729 - acc: 0.9653\n",
      "Epoch 32/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.1694 - acc: 0.9658\n",
      "Epoch 33/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.1662 - acc: 0.9650\n",
      "Epoch 34/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.1616 - acc: 0.9671\n",
      "Epoch 35/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.1598 - acc: 0.9670\n",
      "Epoch 36/1000\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.1560 - acc: 0.9678\n",
      "Epoch 37/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.1521 - acc: 0.9688\n",
      "Epoch 38/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.1488 - acc: 0.9694\n",
      "Epoch 39/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.1464 - acc: 0.9701\n",
      "Epoch 40/1000\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 0.1434 - acc: 0.9703\n",
      "Epoch 41/1000\n",
      "8000/8000 [==============================] - 0s 35us/step - loss: 0.1413 - acc: 0.9710\n",
      "Epoch 42/1000\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.1387 - acc: 0.9718\n",
      "Epoch 43/1000\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.1371 - acc: 0.9718\n",
      "Epoch 44/1000\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.1350 - acc: 0.9716\n",
      "Epoch 45/1000\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.1338 - acc: 0.9721\n",
      "Epoch 46/1000\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.1318 - acc: 0.9723\n",
      "Epoch 47/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.1300 - acc: 0.9728\n",
      "Epoch 48/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.1288 - acc: 0.9723\n",
      "Epoch 49/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.1271 - acc: 0.9721\n",
      "Epoch 50/1000\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.1251 - acc: 0.9741\n",
      "Epoch 51/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.1238 - acc: 0.9738\n",
      "Epoch 52/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.1221 - acc: 0.9739\n",
      "Epoch 53/1000\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.1214 - acc: 0.9741\n",
      "Epoch 54/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.1200 - acc: 0.9744\n",
      "Epoch 55/1000\n",
      "8000/8000 [==============================] - 0s 36us/step - loss: 0.1188 - acc: 0.9746\n",
      "Epoch 56/1000\n",
      "8000/8000 [==============================] - 0s 36us/step - loss: 0.1176 - acc: 0.9748\n",
      "Epoch 57/1000\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.1164 - acc: 0.9749\n",
      "Epoch 58/1000\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.1153 - acc: 0.9754\n",
      "Epoch 59/1000\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.1143 - acc: 0.9751\n",
      "Epoch 60/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.1136 - acc: 0.9751\n",
      "Epoch 61/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.1133 - acc: 0.9750\n",
      "Epoch 62/1000\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.1129 - acc: 0.9753\n",
      "Epoch 63/1000\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.1121 - acc: 0.9749\n",
      "Epoch 64/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.1107 - acc: 0.9754\n",
      "Epoch 65/1000\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.1096 - acc: 0.9759\n",
      "Epoch 66/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.1089 - acc: 0.9760\n",
      "Epoch 67/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.1084 - acc: 0.9756\n",
      "Epoch 68/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.1078 - acc: 0.9760\n",
      "Epoch 69/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.1067 - acc: 0.9761\n",
      "Epoch 70/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.1060 - acc: 0.9761\n",
      "Epoch 71/1000\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.1053 - acc: 0.9763\n",
      "Epoch 72/1000\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.1046 - acc: 0.9768\n",
      "Epoch 73/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.1042 - acc: 0.9765\n",
      "Epoch 74/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.1035 - acc: 0.9769\n",
      "Epoch 75/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.1029 - acc: 0.9773\n",
      "Epoch 76/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.1022 - acc: 0.9771\n",
      "Epoch 77/1000\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.1016 - acc: 0.9771\n",
      "Epoch 78/1000\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.1011 - acc: 0.9771\n",
      "Epoch 79/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.1005 - acc: 0.9774\n",
      "Epoch 80/1000\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.1002 - acc: 0.9774\n",
      "Epoch 81/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0999 - acc: 0.9776\n",
      "Epoch 82/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0991 - acc: 0.9778\n",
      "Epoch 83/1000\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.0984 - acc: 0.9775\n",
      "Epoch 84/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0979 - acc: 0.9775\n",
      "Epoch 85/1000\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.0974 - acc: 0.9778\n",
      "Epoch 86/1000\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.0969 - acc: 0.9774\n",
      "Epoch 87/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0964 - acc: 0.9778\n",
      "Epoch 88/1000\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.0959 - acc: 0.9775\n",
      "Epoch 89/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0955 - acc: 0.9778\n",
      "Epoch 90/1000\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.0950 - acc: 0.9776\n",
      "Epoch 91/1000\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.0946 - acc: 0.9779\n",
      "Epoch 92/1000\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.0941 - acc: 0.9780\n",
      "Epoch 93/1000\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.0938 - acc: 0.9779\n",
      "Epoch 94/1000\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.0933 - acc: 0.9780\n",
      "Epoch 95/1000\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.0930 - acc: 0.9779\n",
      "Epoch 96/1000\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.0926 - acc: 0.9780\n",
      "Epoch 97/1000\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.0921 - acc: 0.9781\n",
      "Epoch 98/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0917 - acc: 0.9780\n",
      "Epoch 99/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0914 - acc: 0.9783\n",
      "Epoch 100/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0910 - acc: 0.9785\n",
      "Epoch 101/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0905 - acc: 0.9783\n",
      "Epoch 102/1000\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.0902 - acc: 0.9793\n",
      "Epoch 103/1000\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.0898 - acc: 0.9793\n",
      "Epoch 104/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0894 - acc: 0.9786\n",
      "Epoch 105/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0891 - acc: 0.9790\n",
      "Epoch 106/1000\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.0887 - acc: 0.9796\n",
      "Epoch 107/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0883 - acc: 0.9796\n",
      "Epoch 108/1000\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.0880 - acc: 0.9800\n",
      "Epoch 109/1000\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.0876 - acc: 0.9795\n",
      "Epoch 110/1000\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.0873 - acc: 0.9800\n",
      "Epoch 111/1000\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.0869 - acc: 0.9801\n",
      "Epoch 112/1000\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.0865 - acc: 0.9796\n",
      "Epoch 113/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0865 - acc: 0.9803\n",
      "Epoch 114/1000\n",
      "8000/8000 [==============================] - 0s 37us/step - loss: 0.0863 - acc: 0.9801\n",
      "Epoch 115/1000\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.0860 - acc: 0.9801\n",
      "Epoch 116/1000\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.0855 - acc: 0.9805\n",
      "Epoch 117/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0850 - acc: 0.9805\n",
      "Epoch 118/1000\n",
      "8000/8000 [==============================] - 0s 38us/step - loss: 0.0847 - acc: 0.9806\n",
      "Epoch 119/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0844 - acc: 0.9806\n",
      "Epoch 120/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0840 - acc: 0.9809\n",
      "Epoch 121/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0837 - acc: 0.9809\n",
      "Epoch 122/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0834 - acc: 0.9810\n",
      "Epoch 123/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0831 - acc: 0.9811\n",
      "Epoch 124/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0827 - acc: 0.9813\n",
      "Epoch 125/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0824 - acc: 0.9811\n",
      "Epoch 126/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0822 - acc: 0.9813\n",
      "Epoch 127/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0819 - acc: 0.9811\n",
      "Epoch 128/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0817 - acc: 0.9810\n",
      "Epoch 129/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0813 - acc: 0.9814\n",
      "Epoch 130/1000\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.0810 - acc: 0.9815\n",
      "Epoch 131/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0807 - acc: 0.9816\n",
      "Epoch 132/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0805 - acc: 0.9816\n",
      "Epoch 133/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0802 - acc: 0.9815\n",
      "Epoch 134/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0800 - acc: 0.9815\n",
      "Epoch 135/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0798 - acc: 0.9813\n",
      "Epoch 136/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0796 - acc: 0.9814\n",
      "Epoch 137/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0790 - acc: 0.9819\n",
      "Epoch 138/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0789 - acc: 0.9819\n",
      "Epoch 139/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0786 - acc: 0.9818\n",
      "Epoch 140/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0783 - acc: 0.9816\n",
      "Epoch 141/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0780 - acc: 0.9819\n",
      "Epoch 142/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0777 - acc: 0.9819\n",
      "Epoch 143/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0774 - acc: 0.9819\n",
      "Epoch 144/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0772 - acc: 0.9821\n",
      "Epoch 145/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0769 - acc: 0.9819\n",
      "Epoch 146/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0767 - acc: 0.9821\n",
      "Epoch 147/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0765 - acc: 0.9821\n",
      "Epoch 148/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0763 - acc: 0.9820\n",
      "Epoch 149/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0759 - acc: 0.9823\n",
      "Epoch 150/1000\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.0757 - acc: 0.9823\n",
      "Epoch 151/1000\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.0755 - acc: 0.9821\n",
      "Epoch 152/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0753 - acc: 0.9823\n",
      "Epoch 153/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0750 - acc: 0.9821\n",
      "Epoch 154/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0748 - acc: 0.9823\n",
      "Epoch 155/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0746 - acc: 0.9825\n",
      "Epoch 156/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0743 - acc: 0.9824\n",
      "Epoch 157/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0742 - acc: 0.9824\n",
      "Epoch 158/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0739 - acc: 0.9824\n",
      "Epoch 159/1000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 0.0765 - acc: 0.981 - 0s 24us/step - loss: 0.0737 - acc: 0.9825\n",
      "Epoch 160/1000\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.0734 - acc: 0.9826\n",
      "Epoch 161/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0732 - acc: 0.9826\n",
      "Epoch 162/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0730 - acc: 0.9826\n",
      "Epoch 163/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0729 - acc: 0.9826\n",
      "Epoch 164/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0726 - acc: 0.9829\n",
      "Epoch 165/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0725 - acc: 0.9829: 0s - loss: 0.0740 - acc: 0.982\n",
      "Epoch 166/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0723 - acc: 0.9830\n",
      "Epoch 167/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0720 - acc: 0.9829\n",
      "Epoch 168/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0718 - acc: 0.9829\n",
      "Epoch 169/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0716 - acc: 0.9833\n",
      "Epoch 170/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0713 - acc: 0.9835\n",
      "Epoch 171/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0712 - acc: 0.9834\n",
      "Epoch 172/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0710 - acc: 0.9835\n",
      "Epoch 173/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0708 - acc: 0.9831\n",
      "Epoch 174/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0706 - acc: 0.9835\n",
      "Epoch 175/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0704 - acc: 0.9833\n",
      "Epoch 176/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0702 - acc: 0.9835\n",
      "Epoch 177/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0700 - acc: 0.9835\n",
      "Epoch 178/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0698 - acc: 0.9836\n",
      "Epoch 179/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0696 - acc: 0.9835\n",
      "Epoch 180/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0695 - acc: 0.9836\n",
      "Epoch 181/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0694 - acc: 0.9834\n",
      "Epoch 182/1000\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.0691 - acc: 0.9836\n",
      "Epoch 183/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0689 - acc: 0.9835\n",
      "Epoch 184/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0687 - acc: 0.9839\n",
      "Epoch 185/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0687 - acc: 0.9838\n",
      "Epoch 186/1000\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.0685 - acc: 0.9838\n",
      "Epoch 187/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0682 - acc: 0.9839\n",
      "Epoch 188/1000\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.0680 - acc: 0.9839\n",
      "Epoch 189/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0678 - acc: 0.9841\n",
      "Epoch 190/1000\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.0676 - acc: 0.9839\n",
      "Epoch 191/1000\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.0674 - acc: 0.9840\n",
      "Epoch 192/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0672 - acc: 0.9840\n",
      "Epoch 193/1000\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.0671 - acc: 0.9841\n",
      "Epoch 194/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0669 - acc: 0.9844\n",
      "Epoch 195/1000\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.0667 - acc: 0.9844\n",
      "Epoch 196/1000\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.0665 - acc: 0.9843\n",
      "Epoch 197/1000\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.0664 - acc: 0.9845\n",
      "Epoch 198/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0662 - acc: 0.9844\n",
      "Epoch 199/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0660 - acc: 0.9845\n",
      "Epoch 200/1000\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.0658 - acc: 0.9846\n",
      "Epoch 201/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0657 - acc: 0.9845\n",
      "Epoch 202/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0655 - acc: 0.9846\n",
      "Epoch 203/1000\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.0653 - acc: 0.9848\n",
      "Epoch 204/1000\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.0652 - acc: 0.9846\n",
      "Epoch 205/1000\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.0650 - acc: 0.9846\n",
      "Epoch 206/1000\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.0649 - acc: 0.9848\n",
      "Epoch 207/1000\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.0647 - acc: 0.9850\n",
      "Epoch 208/1000\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.0645 - acc: 0.9846\n",
      "Epoch 209/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0644 - acc: 0.9849\n",
      "Epoch 210/1000\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.0642 - acc: 0.9851\n",
      "Epoch 211/1000\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.0641 - acc: 0.9850\n",
      "Epoch 212/1000\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.0639 - acc: 0.9849\n",
      "Epoch 213/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0638 - acc: 0.9851\n",
      "Epoch 214/1000\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.0636 - acc: 0.9850\n",
      "Epoch 215/1000\n",
      "8000/8000 [==============================] - 0s 35us/step - loss: 0.0634 - acc: 0.9849\n",
      "Epoch 216/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0633 - acc: 0.9853\n",
      "Epoch 217/1000\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.0632 - acc: 0.9854\n",
      "Epoch 218/1000\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.0630 - acc: 0.9853\n",
      "Epoch 219/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0629 - acc: 0.9855\n",
      "Epoch 220/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0627 - acc: 0.9856\n",
      "Epoch 221/1000\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.0627 - acc: 0.9855\n",
      "Epoch 222/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0625 - acc: 0.9854\n",
      "Epoch 223/1000\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.0625 - acc: 0.9859\n",
      "Epoch 224/1000\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.0621 - acc: 0.9858\n",
      "Epoch 225/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0620 - acc: 0.9856\n",
      "Epoch 226/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0618 - acc: 0.9855\n",
      "Epoch 227/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0616 - acc: 0.9859\n",
      "Epoch 228/1000\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.0616 - acc: 0.9859\n",
      "Epoch 229/1000\n",
      "8000/8000 [==============================] - 0s 36us/step - loss: 0.0614 - acc: 0.9860\n",
      "Epoch 230/1000\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.0612 - acc: 0.9861\n",
      "Epoch 231/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0610 - acc: 0.9859\n",
      "Epoch 232/1000\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.0610 - acc: 0.9863\n",
      "Epoch 233/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0607 - acc: 0.9863\n",
      "Epoch 234/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0610 - acc: 0.9860\n",
      "Epoch 235/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0608 - acc: 0.9860\n",
      "Epoch 236/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0605 - acc: 0.9861\n",
      "Epoch 237/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0607 - acc: 0.9858\n",
      "Epoch 238/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0605 - acc: 0.9860\n",
      "Epoch 239/1000\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.0601 - acc: 0.9859\n",
      "Epoch 240/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0598 - acc: 0.9861\n",
      "Epoch 241/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0597 - acc: 0.9863\n",
      "Epoch 242/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0595 - acc: 0.9863\n",
      "Epoch 243/1000\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.0593 - acc: 0.9864\n",
      "Epoch 244/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0591 - acc: 0.9865\n",
      "Epoch 245/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0590 - acc: 0.9865\n",
      "Epoch 246/1000\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.0588 - acc: 0.9868\n",
      "Epoch 247/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0587 - acc: 0.9865\n",
      "Epoch 248/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0585 - acc: 0.9866\n",
      "Epoch 249/1000\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.0584 - acc: 0.9868\n",
      "Epoch 250/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0583 - acc: 0.9868\n",
      "Epoch 251/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0582 - acc: 0.9869\n",
      "Epoch 252/1000\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.0580 - acc: 0.9870\n",
      "Epoch 253/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0579 - acc: 0.9869\n",
      "Epoch 254/1000\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.0577 - acc: 0.9869\n",
      "Epoch 255/1000\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.0576 - acc: 0.9869\n",
      "Epoch 256/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0575 - acc: 0.9869\n",
      "Epoch 257/1000\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.0573 - acc: 0.9868\n",
      "Epoch 258/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0572 - acc: 0.9868\n",
      "Epoch 259/1000\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.0571 - acc: 0.9870\n",
      "Epoch 260/1000\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.0569 - acc: 0.9869\n",
      "Epoch 261/1000\n",
      "8000/8000 [==============================] - 0s 35us/step - loss: 0.0568 - acc: 0.9869\n",
      "Epoch 262/1000\n",
      "8000/8000 [==============================] - 0s 36us/step - loss: 0.0567 - acc: 0.9868: 0s - loss: 0.0637 - acc: 0.\n",
      "Epoch 263/1000\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 0.0566 - acc: 0.9869\n",
      "Epoch 264/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0564 - acc: 0.9870\n",
      "Epoch 265/1000\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.0563 - acc: 0.9870\n",
      "Epoch 266/1000\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.0562 - acc: 0.9871\n",
      "Epoch 267/1000\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.0560 - acc: 0.9871\n",
      "Epoch 268/1000\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.0559 - acc: 0.9869\n",
      "Epoch 269/1000\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.0558 - acc: 0.9870\n",
      "Epoch 270/1000\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.0557 - acc: 0.9873\n",
      "Epoch 271/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0555 - acc: 0.9874\n",
      "Epoch 272/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0554 - acc: 0.9871\n",
      "Epoch 273/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0553 - acc: 0.9871\n",
      "Epoch 274/1000\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.0552 - acc: 0.9874\n",
      "Epoch 275/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0551 - acc: 0.9871\n",
      "Epoch 276/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0550 - acc: 0.9871\n",
      "Epoch 277/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0548 - acc: 0.9874\n",
      "Epoch 278/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0547 - acc: 0.9874\n",
      "Epoch 279/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0546 - acc: 0.9874\n",
      "Epoch 280/1000\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.0548 - acc: 0.9876\n",
      "Epoch 281/1000\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 0.0545 - acc: 0.9876\n",
      "Epoch 282/1000\n",
      "8000/8000 [==============================] - 0s 35us/step - loss: 0.0546 - acc: 0.9878\n",
      "Epoch 283/1000\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.0544 - acc: 0.9878\n",
      "Epoch 284/1000\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.0542 - acc: 0.9880\n",
      "Epoch 285/1000\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.0540 - acc: 0.9880\n",
      "Epoch 286/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0539 - acc: 0.9880\n",
      "Epoch 287/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0537 - acc: 0.9883\n",
      "Epoch 288/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0535 - acc: 0.9883\n",
      "Epoch 289/1000\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.0534 - acc: 0.9883\n",
      "Epoch 290/1000\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.0533 - acc: 0.9884\n",
      "Epoch 291/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0532 - acc: 0.9883\n",
      "Epoch 292/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0531 - acc: 0.9884\n",
      "Epoch 293/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0529 - acc: 0.9884\n",
      "Epoch 294/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0528 - acc: 0.9884\n",
      "Epoch 295/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0527 - acc: 0.9884\n",
      "Epoch 296/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0526 - acc: 0.9884\n",
      "Epoch 297/1000\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.0525 - acc: 0.9885\n",
      "Epoch 298/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0524 - acc: 0.9886\n",
      "Epoch 299/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0523 - acc: 0.9885\n",
      "Epoch 300/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0523 - acc: 0.9886\n",
      "Epoch 301/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0521 - acc: 0.9886\n",
      "Epoch 302/1000\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.0520 - acc: 0.9889\n",
      "Epoch 303/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0519 - acc: 0.9888\n",
      "Epoch 304/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0518 - acc: 0.9888\n",
      "Epoch 305/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0517 - acc: 0.9889\n",
      "Epoch 306/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0515 - acc: 0.9889\n",
      "Epoch 307/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0514 - acc: 0.9886\n",
      "Epoch 308/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0513 - acc: 0.9889\n",
      "Epoch 309/1000\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.0512 - acc: 0.9889\n",
      "Epoch 310/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0511 - acc: 0.9889\n",
      "Epoch 311/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0510 - acc: 0.9889\n",
      "Epoch 312/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0509 - acc: 0.9889\n",
      "Epoch 313/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0508 - acc: 0.9890\n",
      "Epoch 314/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0507 - acc: 0.9890\n",
      "Epoch 315/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0506 - acc: 0.9891\n",
      "Epoch 316/1000\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.0505 - acc: 0.9891\n",
      "Epoch 317/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0504 - acc: 0.9890\n",
      "Epoch 318/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0503 - acc: 0.9891\n",
      "Epoch 319/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0502 - acc: 0.9890\n",
      "Epoch 320/1000\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.0501 - acc: 0.9893\n",
      "Epoch 321/1000\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.0499 - acc: 0.9891\n",
      "Epoch 322/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0498 - acc: 0.9893\n",
      "Epoch 323/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0498 - acc: 0.9894\n",
      "Epoch 324/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0497 - acc: 0.9893\n",
      "Epoch 325/1000\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.0495 - acc: 0.9893\n",
      "Epoch 326/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0495 - acc: 0.9894\n",
      "Epoch 327/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0494 - acc: 0.9893\n",
      "Epoch 328/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0493 - acc: 0.9894\n",
      "Epoch 329/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0492 - acc: 0.9893\n",
      "Epoch 330/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0491 - acc: 0.9893\n",
      "Epoch 331/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0490 - acc: 0.9894\n",
      "Epoch 332/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0489 - acc: 0.9894\n",
      "Epoch 333/1000\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.0487 - acc: 0.9894\n",
      "Epoch 334/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0486 - acc: 0.9894\n",
      "Epoch 335/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0486 - acc: 0.9894\n",
      "Epoch 336/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0485 - acc: 0.9894\n",
      "Epoch 337/1000\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.0484 - acc: 0.9894\n",
      "Epoch 338/1000\n",
      "8000/8000 [==============================] - 0s 37us/step - loss: 0.0482 - acc: 0.9894\n",
      "Epoch 339/1000\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.0481 - acc: 0.9894\n",
      "Epoch 340/1000\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.0480 - acc: 0.9894\n",
      "Epoch 341/1000\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.0479 - acc: 0.9894\n",
      "Epoch 342/1000\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.0479 - acc: 0.9894\n",
      "Epoch 343/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0478 - acc: 0.9894\n",
      "Epoch 344/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0477 - acc: 0.9894\n",
      "Epoch 345/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0476 - acc: 0.9894\n",
      "Epoch 346/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0475 - acc: 0.9894\n",
      "Epoch 347/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0474 - acc: 0.9894\n",
      "Epoch 348/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0473 - acc: 0.9894\n",
      "Epoch 349/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0472 - acc: 0.9894\n",
      "Epoch 350/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0471 - acc: 0.9894\n",
      "Epoch 351/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0470 - acc: 0.9894\n",
      "Epoch 352/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0470 - acc: 0.9894\n",
      "Epoch 353/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0468 - acc: 0.9894\n",
      "Epoch 354/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0467 - acc: 0.9895\n",
      "Epoch 355/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0467 - acc: 0.9896\n",
      "Epoch 356/1000\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.0466 - acc: 0.9895\n",
      "Epoch 357/1000\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.0465 - acc: 0.9895\n",
      "Epoch 358/1000\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.0464 - acc: 0.9896\n",
      "Epoch 359/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0463 - acc: 0.9896\n",
      "Epoch 360/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0462 - acc: 0.9896\n",
      "Epoch 361/1000\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.0461 - acc: 0.9898\n",
      "Epoch 362/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0461 - acc: 0.9898\n",
      "Epoch 363/1000\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.0460 - acc: 0.9898\n",
      "Epoch 364/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0459 - acc: 0.9899\n",
      "Epoch 365/1000\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.0458 - acc: 0.9899\n",
      "Epoch 366/1000\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.0457 - acc: 0.9896\n",
      "Epoch 367/1000\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.0456 - acc: 0.9898\n",
      "Epoch 368/1000\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.0456 - acc: 0.9898\n",
      "Epoch 369/1000\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.0455 - acc: 0.9898\n",
      "Epoch 370/1000\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.0454 - acc: 0.9900\n",
      "Epoch 371/1000\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.0453 - acc: 0.9900\n",
      "Epoch 372/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0452 - acc: 0.9898\n",
      "Epoch 373/1000\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.0451 - acc: 0.9900\n",
      "Epoch 374/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0450 - acc: 0.9899\n",
      "Epoch 375/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0450 - acc: 0.9899\n",
      "Epoch 376/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0449 - acc: 0.9900\n",
      "Epoch 377/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0448 - acc: 0.9901\n",
      "Epoch 378/1000\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.0447 - acc: 0.9901\n",
      "Epoch 379/1000\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.0446 - acc: 0.9903\n",
      "Epoch 380/1000\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.0445 - acc: 0.9900\n",
      "Epoch 381/1000\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.0445 - acc: 0.9901\n",
      "Epoch 382/1000\n",
      "4000/8000 [==============>...............] - ETA: 0s - loss: 0.0459 - acc: 0.9890"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-14835b9db09f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mloss_and_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\me\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\me\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\me\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\me\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\me\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_in = len(X[0])\n",
    "n_hidden = 200\n",
    "n_out = len(Y[0])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(n_hidden, input_dim=n_in))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Dense(n_out))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=SGD(lr=0.01),\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "epochs = 1000\n",
    "batch_size = 100\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "loss_and_metrics = model.evaluate(X_test, Y_test)\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T04:41:20.081324Z",
     "start_time": "2019-02-21T04:41:20.060336Z"
    }
   },
   "outputs": [],
   "source": [
    "model.add(Dense(n_hidden))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T04:41:20.157273Z",
     "start_time": "2019-02-21T04:41:20.084320Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(n_hidden, input_dim=n_in))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Dense(n_hidden))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Dense(n_hidden))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Dense(n_out))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T04:41:20.163271Z",
     "start_time": "2019-02-21T04:41:20.159275Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T04:45:19.168083Z",
     "start_time": "2019-02-21T04:45:19.133105Z"
    }
   },
   "outputs": [],
   "source": [
    "class DNN(object):\n",
    "    def __init__(self, n_in, n_hiddens, n_out):\n",
    "        #초기화 처리\n",
    "        self.n_in = n_in\n",
    "        self.n_hiddens = n_hiddens\n",
    "        self.n_out = n_out\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self._x = None\n",
    "        self._t = None\n",
    "        self._keep_prop = None\n",
    "        self._sess = None\n",
    "        self._history = {'accuracy': [], 'loss': []}\n",
    "\n",
    "    def weight_variable(self, shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(self, shape):\n",
    "        initial = tf.zeros(shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def inference(self, x, keep_prob):\n",
    "        #모델 정의\n",
    "        for i, n_hidden in enumerate(self.n_hiddens):\n",
    "            if i == 0:\n",
    "                input = x\n",
    "                input_dim = self.n_in\n",
    "            else:\n",
    "                input = output\n",
    "                input_dim = self.n_hiddens[i - 1]\n",
    "\n",
    "            self.weights.append(self.weight_variable([input_dim, n_hidden]))\n",
    "            self.biases.append(self.bias_variable([n_hidden]))\n",
    "\n",
    "            h = tf.nn.relu(\n",
    "                tf.matmul(input, self.weights[-1]) + self.biaases[-1])\n",
    "\n",
    "            output = tf.nn.dropout(h, keep_prob)\n",
    "\n",
    "        self.weights.append(\n",
    "            self.weight_variable([self.n_hiddens[-1], self.n_out]))\n",
    "        self.biases.append(self.bias_variable([self.n_out]))\n",
    "\n",
    "        y = tf.nn.softmax(\n",
    "            tf.matmul(output, self.weights[-1]) + self.biases[-1])\n",
    "        return y\n",
    "\n",
    "    def loss(self, y, t):\n",
    "        cross_entropy = tf.reduc_mean(\n",
    "            -tf.reduce_sum(t * tf.log(y), reduction_indices=[1]))\n",
    "        return cross_entropy0\n",
    "\n",
    "    def training(self, loss):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "        train_step = optimizer.minimize(loss)\n",
    "        return train_step\n",
    "\n",
    "    def accuracy(self, y, t):\n",
    "        correct_prediction - tf.equal(tf.argmax(y, 1), tf.argmax(t, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        return accuracy\n",
    "\n",
    "    def fit(self,\n",
    "            X_train,\n",
    "            Y_train,\n",
    "            epochs=100,\n",
    "            batch_size=100,\n",
    "            p_keep=0.5,\n",
    "            verbose=1):\n",
    "        # 학습처리\n",
    "        x = tf.placeholder(tf.float32, shape=[None, self.n_in])\n",
    "        t = tf.placeholder(tf.float32, shape=[None, self.n_out])\n",
    "        keep_prop = tf.placeholder(tf.float32)\n",
    "\n",
    "        # evaluate()용으로 작성\n",
    "        self._x = x\n",
    "        self._t = t\n",
    "        self._keep_prob = keep_prop\n",
    "\n",
    "        y = self.inference(x, keep_prob)\n",
    "        loss = self.loss(y, t)\n",
    "        train_step = self.training(loss)\n",
    "        accuracy = self.accuracy(y, t)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess = tf.Session()\n",
    "        sess.run(init)\n",
    "\n",
    "        # evalutate()용으로 작성해둔다\n",
    "        self._sess = sess\n",
    "\n",
    "        N_train = len(X_train)\n",
    "        n_batches = N_train // batch_size\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            X_, y_ = shuffle(X_train, Y_train)\n",
    "\n",
    "            for i in range(n_batches):\n",
    "                start = i * batch_size\n",
    "                end = stasrt + batch_size\n",
    "\n",
    "                sess.run(\n",
    "                    train_step,\n",
    "                    feed_dict={\n",
    "                        x: X_[start:end],\n",
    "                        t: Y_[start:end],\n",
    "                        keep_prob: p_keep\n",
    "                    })\n",
    "            loss_ = loss.eval(\n",
    "                session=sess,\n",
    "                feed_dict={\n",
    "                    x: X_train,\n",
    "                    t: Y_train,\n",
    "                    keep_prob: 1.0\n",
    "                })\n",
    "\n",
    "            accuracy_ = accuracy.eval(\n",
    "                session=sess,\n",
    "                feed_dict={\n",
    "                    x: X_train,\n",
    "                    t: Y_train,\n",
    "                    keep_prob: 1.0\n",
    "                })\n",
    "\n",
    "            # 값 기록\n",
    "            self._history['loss'].append(loss_)\n",
    "            self._history['accuracy'].append(accuracy_)\n",
    "\n",
    "            if verbose:\n",
    "                print('epoch:', epoch, ' loss:', loss_, ' accuracy:',\n",
    "                      accuracy_)\n",
    "\n",
    "        return self._history\n",
    "\n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        # 평가처리\n",
    "        return self.accuracy.eval(\n",
    "            session=self._sess,\n",
    "            feed_dict={\n",
    "                self._x: X_test,\n",
    "                self._t: Y_test,\n",
    "                self._keep_prob: 1.0\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T04:41:20.234226Z",
     "start_time": "2019-02-21T04:41:20.198249Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keep_prob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-b50026bab8b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m          \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m          \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m          p_keep=0.5)\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-9157961022d2>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X_train, Y_train, epochs, batch_size, p_keep, verbose)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keep_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keep_prob' is not defined"
     ]
    }
   ],
   "source": [
    "model = DNN(n_in=784,\n",
    "           n_hiddens=[200, 200, 200],\n",
    "           n_out=10)\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "         epochs=50,\n",
    "         batch_size=200,\n",
    "         p_keep=0.5)\n",
    "\n",
    "accuracy = model.evaluate(X_test, Y_test)\n",
    "print('accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=trainsize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "me",
   "language": "python",
   "name": "me"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
